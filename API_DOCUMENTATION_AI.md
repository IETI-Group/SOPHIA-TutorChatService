# Documentaci√≥n de la API del Servicio de IA

Esta documentaci√≥n detalla c√≥mo configurar, ejecutar y utilizar los servicios de Inteligencia Artificial integrados en SOPHIA Coordinator.

---

## üì° Endpoints de la API

URL Base: `/api/v1/ai`

### 1. Chat con IA

Permite interactuar con el modelo de IA.

**Endpoint:** `POST /chat`

#### Cuerpo de la Petici√≥n (Request Body)

| Campo | Tipo | Requerido | Descripci√≥n |
|-------|------|----------|-------------|
| `message` | string | S√≠ | El mensaje actual que se env√≠a a la IA. |
| `model` | string | No | El modelo de IA a utilizar (ej: `llama3.2`, `mistral`). Si no se env√≠a, usa el configurado por defecto. |
| `context` | number[] | No | Array de n√∫meros (tokens) que representa el historial de la conversaci√≥n. |

#### Ejemplo de Petici√≥n

```json
{
  "message": "Hola, ¬øc√≥mo puedo estructurar un curso de Python?",
  "model": "llama3.2", // Opcional
  "context": [] // Opcional, historial de conversaci√≥n
}
```

#### Respuesta

Retorna la respuesta generada por el modelo de IA.

#### ¬øQu√© es el `context`?
El campo `context` es un array de n√∫meros que el modelo genera despu√©s de cada respuesta. Este array codifica toda la conversaci√≥n previa. Para mantener la memoria del chat, debes guardar este array y enviarlo de vuelta en la siguiente petici√≥n.

#### Ejemplo de Flujo de Conversaci√≥n

**Paso 1: Primera Pregunta (Sin contexto)**
```json
{
  "message": "¬øPor qu√© el cielo es azul?"
}
```

**Respuesta del Servidor:**
```json
{
  "response": "El cielo es azul debido a la dispersi√≥n de Rayleigh...",
  "context": [123, 456, 789, ...] // <--- Guarda esto
}
```

**Paso 2: Segunda Pregunta (Con contexto)**
```json
{
  "message": "¬øY por qu√© se pone rojo al atardecer?",
  "context": [123, 456, 789, ...] // <--- Env√≠a lo que recibiste antes
}
```

**Respuesta del Servidor:**
```json
{
  "response": "Al atardecer, la luz recorre m√°s distancia...",
  "context": [123, 456, 789, 1011, 1213, ...] // <--- Nuevo contexto actualizado
}
```

#### Respuesta Exitosa

**C√≥digo:** `200 OK`

```json
{
  "response": "Texto de la respuesta...",
  "context": [1, 2, 3, ...]
}
```

#### Respuesta de Error

**C√≥digo:** `400 Bad Request`
```json
{
  "success": false,
  "error": "Missing required field: message",
  "timestamp": "2023-10-27T10:00:00.000Z"
}
```

---

### 2. Asistente de Cursos

Genera un esquema de curso estructurado y profesional basado en una idea y pautas de estilo.

**Endpoint:** `POST /course-assistant`

#### Cuerpo de la Petici√≥n

| Campo | Tipo | Requerido | Descripci√≥n |
|-------|------|----------|-------------|
| `idea` | string | S√≠ | El concepto central o tema del curso. |
| `guide` | string | S√≠ | Pautas estructurales, audiencia objetivo o requisitos espec√≠ficos. |
| `model` | string | No | El modelo de IA a utilizar (ej: `llama3.2`, `mistral`). Si no se env√≠a, usa el configurado por defecto. |

#### Ejemplo de Petici√≥n

```json
{
  "idea": "Introducci√≥n a la Programaci√≥n en Python para Ciencia de Datos",
  "guide": "La audiencia objetivo son principiantes. Incluir 4 m√≥dulos principales. Enfocarse en ejemplos pr√°cticos."
}
```

#### Respuesta Exitosa

**C√≥digo:** `200 OK`

```json
{
  "response": "T√≠tulo del Curso: Python para Ciencia de Datos\n\nM√≥dulo 1: Fundamentos de Python\n- Lecci√≥n 1.1: Instalaci√≥n y Configuraci√≥n\n- Lecci√≥n 1.2: Variables y Tipos de Datos..."
}
```

#### Respuesta de Error

**C√≥digo:** `400 Bad Request`
```json
{
  "success": false,
  "error": "Missing required field: idea",
  "timestamp": "2023-10-27T10:00:00.000Z"
}
```


## üõ†Ô∏è Prerrequisitos y Configuraci√≥n

Para que el servicio de IA funcione correctamente, es necesario tener **Ollama** instalado y ejecut√°ndose localmente (o en un servidor accesible).

### 1. Instalar Ollama
Ollama es la herramienta que nos permite ejecutar modelos de lenguaje (LLMs) localmente.
- **Descargar:** Visita [ollama.com](https://ollama.com) y descarga la versi√≥n para tu sistema operativo.
- **Instalar:** Sigue las instrucciones del instalador.

### 2. Descargar el Modelo
El proyecto est√° configurado para usar un modelo espec√≠fico (definido en el archivo `.env`). Debes asegurarte de tener ese modelo descargado en Ollama.

Por defecto, si tu `.env` dice `OLLAMA_MODEL=llama3.2`, ejecuta en tu terminal:
```bash
ollama pull llama3.2
```
*Nota: Verifica la variable `OLLAMA_MODEL` en tu archivo `.env` para saber qu√© modelo descargar.*

### 3. Configuraci√≥n de Variables de Entorno (.env)
Aseg√∫rate de que tu archivo `.env` tenga las siguientes variables configuradas correctamente:

```dotenv
# Configuraci√≥n de IA
OLLAMA_HOST=http://127.0.0.1:11434  # URL donde corre Ollama (por defecto es esta)
OLLAMA_MODEL=llama3.2               # El modelo a utilizar (ej: llama3.2, llama2, mistral)
```

### 4. Verificar que Ollama est√° corriendo
Antes de iniciar el servidor, aseg√∫rate de que Ollama est√© activo. Puedes verificarlo entrando a `http://127.0.0.1:11434` en tu navegador (deber√≠a decir "Ollama is running").

### 5. verificar que este corriendo el servicio ejecutando
```bash
pnpm dev
```

